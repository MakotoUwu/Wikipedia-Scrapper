### Wikipedia Scraperüï∏Ô∏èüï∏Ô∏èüï∏Ô∏è

## Description

This project is about creating a scraper that can retrieve some information from an API and leverage it to scrape a website that does not provide an API. 

The goal is to query an API for a list of countries and their past leaders, and then extract and sanitize their short bio from Wikipedia. Finally, the data is saved to disk as a JSON file.

This project is part of the AI Bootcamp at BeCode.org.

## Installation

To run this project, you need to have Python 3 installed on your machine. You also need to install the following libraries and modules:

- requests

- json

- beautifulsoup4

- re

You can install them using pip or conda.

## Usage

To use this project, you first need to clone the repository. You can do this using the following commands:

```git clone https://github.com/your_username/wikipedia_scraper.git```
```cd wikipedia_scraper```

Then, run the `main.py` file using the following command:

```python3 main.py```

## Timeline

This project took ~3 days to complete

### Personal situation

This project was done by me as a solo challenge. I learned how to use requests, beautifulsoup4 and re modules to scrape data from websites and APIs. I also learned how to use json module to save and load data. I faced some challenges with finding the right regular expressions to sanitize the text and handling the cookie errors from the API. I solved them by using online tools and documentation. I enjoyed this project because it was fun and interesting to work with real-world data sources.


